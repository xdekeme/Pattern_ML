{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 1 - Nearest Neighbors and Decision Trees\n",
    "\n",
    "## Lab objectives\n",
    "\n",
    "* Classification with decision trees and random forests.\n",
    "* Cross-validation and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading training data\n",
      "Pre-loading test data\n"
     ]
    }
   ],
   "source": [
    "from lab_tools import CIFAR10, get_hog_image\n",
    "\n",
    "dataset = CIFAR10('./CIFAR10/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Nearest Neighbor\n",
    "\n",
    "The following example uses the Nearest Neighbor algorithm on the Histogram of Gradient decriptors in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;KNeighborsClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">?<span>Documentation for KNeighborsClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>KNeighborsClassifier(n_neighbors=1)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "clf.fit(dataset.train['hog'], dataset.train['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the **descriptive performance** of this classifier ?\n",
    "* Modify the code to estimate the **predictive performance**.\n",
    "* Use cross-validation to find the best hyper-parameters for this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive 1.0\n",
      "Predictive 0.692\n"
     ]
    }
   ],
   "source": [
    "# -- Your code here -- #\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pred = clf.predict(dataset.train['hog'])\n",
    "score = accuracy_score(dataset.train['labels'], pred)\n",
    "print(f\"Descriptive {score}\") #Descriptive based on the training data\n",
    "\n",
    "split_val = 0.1\n",
    "len_dataset = int(split_val*len(dataset.train[\"hog\"]))\n",
    "train_X = dataset.train[\"hog\"][:-len_dataset]\n",
    "train_Y = dataset.train[\"labels\"][:-len_dataset]\n",
    "val_X = dataset.train[\"hog\"][-len_dataset:]\n",
    "val_Y = dataset.train[\"labels\"][-len_dataset:]\n",
    "\n",
    "clf2 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2.fit(train_X, train_Y)\n",
    "pred2 = clf2.predict(val_X)\n",
    "score2 = accuracy_score(val_Y, pred2)\n",
    "print(f\"Predictive {score2}\") #Predictive based on the testing/validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking different parameters on the raw dataset train (distance metric, number of neighorbs + weighted distance)\n",
    "#Check l'algo en plus ??\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "X, y = dataset.train[\"hog\"], dataset.train[\"labels\"]\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "number_neighbour = 1\n",
    "distance_list = [\"cityblock\", \"cosine\", \"euclidean\", \"haversine\", \"l1\", \"l2\", \"manhattan\", \"nan_euclidean\"] #Pas haversine (only 2D)\n",
    "weighted_distance_list = [\"uniform\", \"distance\"] \n",
    "best_mean = 0\n",
    "best_distance = None\n",
    "best_weight = None \n",
    "best_number_neighbour = 99999\n",
    "\n",
    "\n",
    "while number_neighbour < 20:\n",
    "    for distance in distance_list:\n",
    "        for weight in weighted_distance_list:\n",
    "            clf_test = KNeighborsClassifier(n_neighbors=number_neighbour, weights=weight, metric=distance)\n",
    "            score_cross = cross_val_score(clf_test, X, y, cv=skf)\n",
    "            mean = np.mean(score_cross)\n",
    "            if mean > best_mean:\n",
    "                best_mean = mean \n",
    "                best_distance = distance\n",
    "                best_weight = weight \n",
    "                best_number_neighbour = number_neighbour\n",
    "\n",
    "    number_neighbour += 1          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model - Mean: 0.7804666666666666 Distance metric: cosine Weight: distance Number of neighours: 11\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best model - Mean: {best_mean} Distance metric: {best_distance} Weight: {best_weight} Number of neighours: {best_number_neighbour}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 266 candidates, totalling 1330 fits\n",
      "Meilleure précision obtenue : 0.7804666666666666\n",
      "Meilleurs paramètres : {'metric': 'cosine', 'n_neighbors': 11, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X, y = dataset.train[\"hog\"], dataset.train[\"labels\"]\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': np.arange(1, 20),\n",
    "    'metric': [\"cityblock\", \"cosine\", \"euclidean\", \"l1\", \"l2\", \"manhattan\", \"nan_euclidean\"],\n",
    "    'weights': [\"uniform\", \"distance\"]\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=skf, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Exécution de la recherche sur grille\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Meilleurs résultats\n",
    "print(\"Meilleure précision obtenue :\", grid_search.best_score_)\n",
    "print(\"Meilleurs paramètres :\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of principal components: 104\n",
      "Variance explained : 0.8012945313137937\n"
     ]
    }
   ],
   "source": [
    "#Checking different parameters after removing the extra dimension\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.80)\n",
    "X_pca = pca.fit_transform(dataset.train[\"hog\"])\n",
    "\n",
    "print(\"Number of principal components:\", pca.n_components_)\n",
    "print(\"Variance explained :\", np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Meilleure précision obtenue : 0.7765333333333333\n",
      "Meilleurs paramètres : {'metric': 'cosine', 'n_neighbors': 15, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "X, y = dataset.train[\"hog\"], dataset.train[\"labels\"]\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [1, 3, 5, 11, 15],\n",
    "    'metric': [\"cityblock\", \"cosine\", \"euclidean\", \"manhattan\"],\n",
    "    'weights': [\"uniform\", \"distance\"]\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=skf, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Exécution de la recherche sur grille\n",
    "grid_search.fit(X_pca, y)\n",
    "\n",
    "# Meilleurs résultats\n",
    "print(\"Meilleure précision obtenue :\", grid_search.best_score_)\n",
    "print(\"Meilleurs paramètres :\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive best parameters (raw data): 0.7876666666666666\n",
      "[[755 173  72]\n",
      " [ 81 772 147]\n",
      " [ 24 140 836]]\n",
      "Predictive best parameters (after PCA): 0.802\n",
      "[[847 120  33]\n",
      " [154 741 105]\n",
      " [ 60 122 818]]\n"
     ]
    }
   ],
   "source": [
    "#Comparison of results based on the two hyper-parameters found\n",
    "clf_model1 = KNeighborsClassifier(n_neighbors=11, weights=\"distance\", metric=\"cosine\")\n",
    "clf_model1.fit(dataset.train['hog'], dataset.train['labels'])\n",
    "pred_model1 = clf_model1.predict(dataset.test[\"hog\"])\n",
    "score_model1 = accuracy_score(dataset.test[\"labels\"], pred_model1)\n",
    "print(f\"Predictive best parameters (raw data): {score_model1}\") #Predictive based on the testing/validation data\n",
    "cm_model1 = confusion_matrix(dataset.test[\"labels\"], pred_model1)\n",
    "print(cm_model1)\n",
    "\n",
    "pca = PCA(n_components=0.80)  \n",
    "X_train_pca = pca.fit_transform(dataset.train['hog'])\n",
    "X_test_pca = pca.transform(dataset.test['hog'])\n",
    "clf_model2 = KNeighborsClassifier(n_neighbors=15, weights=\"distance\", metric=\"cosine\")\n",
    "clf_model2.fit(X_train_pca, dataset.train['labels'])\n",
    "pred_model2 = clf_model2.predict(X_test_pca)\n",
    "score_model2 = accuracy_score(dataset.test[\"labels\"], pred_model2)\n",
    "print(f\"Predictive best parameters (after PCA): {score_model2}\") #Predictive based on the testing/validation data\n",
    "cm_model2 = confusion_matrix(dataset.test[\"labels\"], pred_model2)\n",
    "print(cm_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "Best results standard:  0.7872\n",
      "Best parameters: {'metric': 'cityblock', 'n_neighbors': 17, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "#Standardiser les données ? Ou pas ? (soustraction moyenne et division par l'ecart-type pour chaque feature)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = dataset.train[\"hog\"], dataset.train[\"labels\"]\n",
    "scaler = StandardScaler()\n",
    "X_standard = scaler.fit_transform(X)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [11, 15, 17, 19],\n",
    "    'metric': [\"cityblock\", \"cosine\", \"euclidean\", \"manhattan\"],\n",
    "    'weights': [\"uniform\", \"distance\"]\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=skf, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Exécution de la recherche sur grille\n",
    "grid_search.fit(X_standard, y)\n",
    "\n",
    "# Meilleurs résultats\n",
    "print(\"Best results standard: \", grid_search.best_score_)\n",
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "Best results standard + PCA:  0.7769333333333333\n",
      "Best parameters: {'metric': 'cosine', 'n_neighbors': 15, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = dataset.train[\"hog\"], dataset.train[\"labels\"]\n",
    "scaler = StandardScaler()\n",
    "X_standard = scaler.fit_transform(X)\n",
    "X_pca = pca.fit_transform(X_standard)\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [11, 15, 17, 19],\n",
    "    'metric': [\"cityblock\", \"cosine\", \"euclidean\", \"manhattan\"],\n",
    "    'weights': [\"uniform\", \"distance\"]\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=skf, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Exécution de la recherche sur grille\n",
    "grid_search.fit(X_pca, y)\n",
    "\n",
    "# Meilleurs résultats\n",
    "print(\"Best results standard + PCA: \", grid_search.best_score_)\n",
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive best parameters standard: 0.785\n",
      "[[758 188  54]\n",
      " [ 91 792 117]\n",
      " [ 23 172 805]]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(dataset.train['hog'])\n",
    "X_test = scaler.fit_transform(dataset.test['hog'])\n",
    "\n",
    "clf_model_std = KNeighborsClassifier(n_neighbors=17, weights=\"uniform\", metric=\"cityblock\")\n",
    "clf_model_std.fit(X_train, dataset.train['labels'])\n",
    "pred_model_std = clf_model_std.predict(X_test)\n",
    "score_model_std = accuracy_score(dataset.test[\"labels\"], pred_model_std)\n",
    "print(f\"Predictive best parameters standard: {score_model_std}\") #Predictive based on the testing/validation data\n",
    "cm_model_std = confusion_matrix(dataset.test[\"labels\"], pred_model_std)\n",
    "print(cm_model_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive best parameters standard: 0.7956666666666666\n",
      "[[851 119  30]\n",
      " [162 731 107]\n",
      " [ 56 139 805]]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=0.80)  \n",
    "\n",
    "X_train = scaler.fit_transform(dataset.train['hog'])\n",
    "X_train= pca.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(dataset.test['hog'])\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "clf_model_std = KNeighborsClassifier(n_neighbors=15, weights=\"distance\", metric=\"cosine\")\n",
    "clf_model_std.fit(X_train, dataset.train['labels'])\n",
    "pred_model_std = clf_model_std.predict(X_test)\n",
    "score_model_std = accuracy_score(dataset.test[\"labels\"], pred_model_std)\n",
    "print(f\"Predictive best parameters standard: {score_model_std}\") #Predictive based on the testing/validation data\n",
    "cm_model_std = confusion_matrix(dataset.test[\"labels\"], pred_model_std)\n",
    "print(cm_model_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision Trees\n",
    "\n",
    "[Decision Trees](http://scikit-learn.org/stable/modules/tree.html#tree) classify the data by splitting the feature space according to simple, single-feature rules. Scikit-learn uses the [CART](https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29) algorithm for [its implementation](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) of the classifier. \n",
    "\n",
    "* **Create a simple Decision Tree classifier** using scikit-learn and train it on the HoG training set.\n",
    "* Use cross-validation to find the best hyper-paramters for this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive: 0.576\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# --- Your code here --- #\n",
    "clf_tree = tree.DecisionTreeClassifier()\n",
    "clf_tree.fit(train_X, train_Y)\n",
    "pred_tree = clf_tree.predict(val_X)\n",
    "score_tree = accuracy_score(val_Y, pred_tree)\n",
    "print(f\"Predictive: {score_tree}\") #Predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
      "360 fits failed out of a total of 600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got <class 'int'> instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got <class 'float'> instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'criterion' parameter of DecisionTreeClassifier must be a str among {'entropy', 'log_loss', 'gini'}. Got '“entropy”' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.57753333 0.5348\n",
      " 0.5574     0.48653333 0.59586667 0.59153333        nan        nan\n",
      "        nan        nan 0.55813333 0.54213333 0.54466667 0.52173333\n",
      " 0.5822     0.576             nan        nan        nan        nan\n",
      " 0.55226667 0.53786667 0.53986667 0.52153333 0.57246667 0.55766667\n",
      "        nan        nan        nan        nan 0.54846667 0.52633333\n",
      " 0.53626667 0.51186667 0.56933333 0.5574            nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.5686     0.52433333 0.55673333 0.51253333 0.59153333 0.57613333\n",
      "        nan        nan        nan        nan 0.5484     0.53853333\n",
      " 0.54386667 0.5256     0.57826667 0.567             nan        nan\n",
      "        nan        nan 0.5484     0.52693333 0.54       0.52133333\n",
      " 0.5746     0.556             nan        nan        nan        nan\n",
      " 0.546      0.52513333 0.5376     0.51366667 0.576      0.55673333]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best result tree:  0.5958666666666667\n",
      "Best parameters tree:  {'criterion': 'gini', 'max_depth': 10, 'max_features': None, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "#Finding parameters: slip criterion, depth, features to consider per split, pruning + pre-processing (PCA)\n",
    "\n",
    "X, y = dataset.train[\"hog\"], dataset.train[\"labels\"]\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': [\"gini\", \"“entropy”\", \"log_loss\"],\n",
    "    'splitter': [\"best\", \"random\"],\n",
    "    'max_depth': [10, 15, 20, 25],\n",
    "    \"max_features\" : [int, float, \"sqrt\", \"log2\", None]\n",
    "}\n",
    "\n",
    "clf_tree = tree.DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(clf_tree, param_grid, cv=skf, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Exécution de la recherche sur grille\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Meilleurs résultats\n",
    "print(\"Best result tree: \", grid_search.best_score_)\n",
    "print(\"Best parameters tree: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
      "360 fits failed out of a total of 600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got <class 'int'> instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got <class 'float'> instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'criterion' parameter of DecisionTreeClassifier must be a str among {'entropy', 'log_loss', 'gini'}. Got '“entropy”' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/xavierdekeme/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.5852     0.4884\n",
      " 0.5844     0.46353333 0.65873333 0.63766667        nan        nan\n",
      "        nan        nan 0.57853333 0.50426667 0.57553333 0.4564\n",
      " 0.6512     0.6382            nan        nan        nan        nan\n",
      " 0.58906667 0.50753333 0.55093333 0.5118     0.6326     0.617\n",
      "        nan        nan        nan        nan 0.5674     0.54213333\n",
      " 0.5564     0.4898     0.62293333 0.61106667        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.60506667 0.49553333 0.532      0.45793333 0.65146667 0.6402\n",
      "        nan        nan        nan        nan 0.59173333 0.5416\n",
      " 0.55593333 0.47053333 0.6392     0.64273333        nan        nan\n",
      "        nan        nan 0.59106667 0.53913333 0.56586667 0.50866667\n",
      " 0.63006667 0.62773333        nan        nan        nan        nan\n",
      " 0.58266667 0.52066667 0.5506     0.50773333 0.62366667 0.6002    ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best result tree + PCA:  0.6587333333333334\n",
      "Best parameters tree:  {'criterion': 'gini', 'max_depth': 10, 'max_features': None, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=0.80)  \n",
    "X, y = dataset.train[\"hog\"], dataset.train[\"labels\"]\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': [\"gini\", \"“entropy”\", \"log_loss\"],\n",
    "    'splitter': [\"best\", \"random\"],\n",
    "    'max_depth': [10, 12, 15, 20],\n",
    "    \"max_features\" : [int, float, \"sqrt\", \"log2\", None]\n",
    "}\n",
    "\n",
    "clf_tree = tree.DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(clf_tree, param_grid, cv=skf, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Exécution de la recherche sur grille\n",
    "grid_search.fit(X_pca, y)\n",
    "\n",
    "# Meilleurs résultats\n",
    "print(\"Best result tree + PCA: \", grid_search.best_score_)\n",
    "print(\"Best parameters tree: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive best parameters tree (raw data): 0.605\n",
      "[[603 251 146]\n",
      " [153 630 217]\n",
      " [118 300 582]]\n",
      "Predictive best parameters tree (after PCA): 0.652\n",
      "[[645 240 115]\n",
      " [154 625 221]\n",
      " [ 75 239 686]]\n"
     ]
    }
   ],
   "source": [
    "#Comparison of results based on the two hyper-parameters found\n",
    "clf_model_tree1 = tree.DecisionTreeClassifier(criterion=\"gini\", max_depth=10, splitter=\"best\")\n",
    "clf_model_tree1.fit(dataset.train['hog'], dataset.train['labels'])\n",
    "pred_model_tree1 = clf_model_tree1.predict(dataset.test[\"hog\"])\n",
    "score_model_tree1 = accuracy_score(dataset.test[\"labels\"], pred_model_tree1)\n",
    "print(f\"Predictive best parameters tree (raw data): {score_model_tree1}\") #Predictive based on the testing/validation data\n",
    "cm_model_tree1 = confusion_matrix(dataset.test[\"labels\"], pred_model_tree1)\n",
    "print(cm_model_tree1)\n",
    "\n",
    "pca = PCA(n_components=0.80)  \n",
    "X_train_pca = pca.fit_transform(dataset.train['hog'])\n",
    "X_test_pca = pca.transform(dataset.test['hog'])\n",
    "clf_model_tree2 = tree.DecisionTreeClassifier(criterion=\"gini\", max_depth=10, splitter=\"best\")\n",
    "clf_model_tree2.fit(X_train_pca, dataset.train['labels'])\n",
    "pred_model_tree2 = clf_model_tree2.predict(X_test_pca)\n",
    "score_model_tree2 = accuracy_score(dataset.test[\"labels\"], pred_model_tree2)\n",
    "print(f\"Predictive best parameters tree (after PCA): {score_model_tree2}\") #Predictive based on the testing/validation data\n",
    "cm_model_tree2 = confusion_matrix(dataset.test[\"labels\"], pred_model_tree2)\n",
    "print(cm_model_tree2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forests\n",
    "\n",
    "[Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) classifiers use multiple decision trees trained on \"weaker\" datasets (less data and/or less features), averaging the results so as to reduce over-fitting.\n",
    "\n",
    "* Use scikit-learn to **create a Random Forest classifier** on the CIFAR data. \n",
    "* Use cross-validation to find the best hyper-paramters for this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive forest: 0.7713333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "# --- Your code here --- #\n",
    "clf_forest = ensemble.RandomForestClassifier()\n",
    "clf_forest.fit(train_X, train_Y)\n",
    "pred_forest = clf_forest.predict(val_X)\n",
    "score_forest = accuracy_score(val_Y, pred_forest)\n",
    "print(f\"Predictive forest: {score_forest}\") #Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best result forest:  0.7375999999999999\n",
      "Best parameters forest:  {'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "X, y = dataset.train[\"hog\"], dataset.train[\"labels\"]\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 50, 75, 100]\n",
    "}\n",
    "\n",
    "clf_forest = ensemble.RandomForestClassifier(criterion=\"gini\", max_depth=10, max_features=None)\n",
    "grid_search = GridSearchCV(clf_forest, param_grid, cv=skf, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Exécution de la recherche sur grille\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Meilleurs résultats\n",
    "print(\"Best result forest: \", grid_search.best_score_)\n",
    "print(\"Best parameters forest: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best result forest:  0.7248\n",
      "Best parameters forest:  {'n_estimators': 75}\n"
     ]
    }
   ],
   "source": [
    "X, y = dataset.train[\"hog\"], dataset.train[\"labels\"]\n",
    "X_train_pca = pca.fit_transform(dataset.train['hog'])\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [20, 50, 75]\n",
    "}\n",
    "\n",
    "clf_forest = ensemble.RandomForestClassifier(criterion=\"gini\", max_depth=10, max_features=None)\n",
    "grid_search = GridSearchCV(clf_forest, param_grid, cv=skf, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Exécution de la recherche sur grille\n",
    "grid_search.fit(X_train_pca, y)\n",
    "\n",
    "# Meilleurs résultats\n",
    "print(\"Best result forest: \", grid_search.best_score_)\n",
    "print(\"Best parameters forest: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive best parameters tree (raw data): 0.732\n",
      "[[755 188  57]\n",
      " [127 717 156]\n",
      " [ 75 201 724]]\n",
      "Predictive best parameters tree (after PCA): 0.7113333333333334\n",
      "[[723 194  83]\n",
      " [134 671 195]\n",
      " [ 55 205 740]]\n"
     ]
    }
   ],
   "source": [
    "#Comparison of results based on the two hyper-parameters found\n",
    "clf_model_tree1 = ensemble.RandomForestClassifier(criterion=\"gini\", max_depth=10, max_features=None, n_estimators=50)\n",
    "clf_model_tree1.fit(dataset.train['hog'], dataset.train['labels'])\n",
    "pred_model_tree1 = clf_model_tree1.predict(dataset.test[\"hog\"])\n",
    "score_model_tree1 = accuracy_score(dataset.test[\"labels\"], pred_model_tree1)\n",
    "print(f\"Predictive best parameters tree (raw data): {score_model_tree1}\") #Predictive based on the testing/validation data\n",
    "cm_model_tree1 = confusion_matrix(dataset.test[\"labels\"], pred_model_tree1)\n",
    "print(cm_model_tree1)\n",
    "\n",
    "pca = PCA(n_components=0.80)  \n",
    "X_train_pca = pca.fit_transform(dataset.train['hog'])\n",
    "X_test_pca = pca.transform(dataset.test['hog'])\n",
    "clf_model_tree2 = ensemble.RandomForestClassifier(criterion=\"gini\", max_depth=10, max_features=None, n_estimators=75)\n",
    "clf_model_tree2.fit(X_train_pca, dataset.train['labels'])\n",
    "pred_model_tree2 = clf_model_tree2.predict(X_test_pca)\n",
    "score_model_tree2 = accuracy_score(dataset.test[\"labels\"], pred_model_tree2)\n",
    "print(f\"Predictive best parameters tree (after PCA): {score_model_tree2}\") #Predictive based on the testing/validation data\n",
    "cm_model_tree2 = confusion_matrix(dataset.test[\"labels\"], pred_model_tree2)\n",
    "print(cm_model_tree2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
